{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview processing and insertion into test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import getpass\n",
    "import os\n",
    "import docx2json\n",
    "import json\n",
    "import copy\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.tokenize in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.tokenize - NLTK Tokenizer Package\n",
      "\n",
      "DESCRIPTION\n",
      "    Tokenizers divide strings into lists of substrings.  For example,\n",
      "    tokenizers can be used to find the words and punctuation in a string:\n",
      "    \n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
      "        ... two of them.\\n\\nThanks.'''\n",
      "        >>> word_tokenize(s)\n",
      "        ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',\n",
      "        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "    \n",
      "    This particular tokenizer requires the Punkt sentence tokenization\n",
      "    models to be installed. NLTK also provides a simpler,\n",
      "    regular-expression based tokenizer, which splits text on whitespace\n",
      "    and punctuation:\n",
      "    \n",
      "        >>> from nltk.tokenize import wordpunct_tokenize\n",
      "        >>> wordpunct_tokenize(s)\n",
      "        ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',\n",
      "        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "    \n",
      "    We can also operate at the level of sentences, using the sentence\n",
      "    tokenizer directly as follows:\n",
      "    \n",
      "        >>> from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "        >>> sent_tokenize(s)\n",
      "        ['Good muffins cost $3.88\\nin New York.', 'Please buy me\\ntwo of them.', 'Thanks.']\n",
      "        >>> [word_tokenize(t) for t in sent_tokenize(s)]\n",
      "        [['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
      "        ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]\n",
      "    \n",
      "    Caution: when tokenizing a Unicode string, make sure you are not\n",
      "    using an encoded version of the string (it may be necessary to\n",
      "    decode it first, e.g. with ``s.decode(\"utf8\")``.\n",
      "    \n",
      "    NLTK tokenizers can produce token-spans, represented as tuples of integers\n",
      "    having the same semantics as string slices, to support efficient comparison\n",
      "    of tokenizers.  (These methods are implemented as generators.)\n",
      "    \n",
      "        >>> from nltk.tokenize import WhitespaceTokenizer\n",
      "        >>> list(WhitespaceTokenizer().span_tokenize(s))\n",
      "        [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),\n",
      "        (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]\n",
      "    \n",
      "    There are numerous ways to tokenize text.  If you need more control over\n",
      "    tokenization, see the other methods provided in this package.\n",
      "    \n",
      "    For further information, please see Chapter 3 of the NLTK book.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    casual\n",
      "    mwe\n",
      "    nist\n",
      "    punkt\n",
      "    regexp\n",
      "    repp\n",
      "    sexpr\n",
      "    simple\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    texttiling\n",
      "    toktok\n",
      "    treebank\n",
      "    util\n",
      "\n",
      "FUNCTIONS\n",
      "    sent_tokenize(text, language='english')\n",
      "        Return a sentence-tokenized copy of *text*,\n",
      "        using NLTK's recommended sentence tokenizer\n",
      "        (currently :class:`.PunktSentenceTokenizer`\n",
      "        for the specified language).\n",
      "        \n",
      "        :param text: text to split into sentences\n",
      "        :param language: the model name in the Punkt corpus\n",
      "    \n",
      "    word_tokenize(text, language='english', preserve_line=False)\n",
      "        Return a tokenized copy of *text*,\n",
      "        using NLTK's recommended word tokenizer\n",
      "        (currently an improved :class:`.TreebankWordTokenizer`\n",
      "        along with :class:`.PunktSentenceTokenizer`\n",
      "        for the specified language).\n",
      "        \n",
      "        :param text: text to split into words\n",
      "        :type text: str\n",
      "        :param language: the model name in the Punkt corpus\n",
      "        :type language: str\n",
      "        :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
      "        :type preserve_line: bool\n",
      "\n",
      "DATA\n",
      "    improved_close_quote_regex = re.compile('([»”’])')\n",
      "    improved_open_quote_regex = re.compile('([«“‘„]|[`]+)')\n",
      "    improved_open_single_quote_regex = re.compile(\"(?i)(\\\\')(?!re|ve|ll|m|...\n",
      "    improved_punct_regex = re.compile('([^\\\\.])(\\\\.)([\\\\]\\\\)}>\"\\\\\\'»”’ ]*)...\n",
      "\n",
      "FILE\n",
      "    /home/andre/anaconda3/envs/icgeo/lib/python3.7/site-packages/nltk/tokenize/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Host:  localhost\n",
      "Port:  5432\n",
      "DatabaseName:  icgeo\n",
      "User:  andre\n",
      "Password:  ···········\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"host=%s port=%s dbname=%s user=%s password=%s\" \n",
    "                        % ((str(input(\"Host: \"))).strip(), (str(input(\"Port: \"))).strip(), (str(input(\"DatabaseName: \"))).strip(), \n",
    "                           (str(input(\"User: \"))).strip(), getpass.getpass(\"Password: \")))\n",
    "cur = conn.cursor()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing interviews with simple pattern recognition / standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviews (number of documents = 18):\n",
      "[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 41, 42]\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/19 - Camila Ferezin do Amarante ( Ginastica ritmica ).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/20 - Natália Scherer Eidt (Ginastica Ritmica) .docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/21 - Flavia Cristina de Faria (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/22 - Alessandra Ferezin Guidugli (Ginástica Rítmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/23 - Thalita Nakadomari (Ginastica  Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/24 -  Dayane Camilo.docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/25 - larissa barata (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/26 - Fernanda Cavalieri (Ginástica Ritimica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/27 - Ana Maria Maciel (Ginastica Ritimica ).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/28 - Tayanne Coelho Montovanelli (ginastica-ritmica) .docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/29 - Jeniffer Celeste (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/30 - Luana Faro (Ginástica Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/31 -  Daniela Aleixo.docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/32 - Luisa Harumi Matsuo (Ginastica ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/33 - Marcela Oliveira (Ginástica Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/34 - Nicole Romme Muller (Ginastica  Ritimica ).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/41 - Rosane Favilla (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic-geo/refactored/source-files/GRD Pronto/Entrevistas Completas/42 - Marta Cristina Schonhorst (ginastica-ritmica).docx\n"
     ]
    }
   ],
   "source": [
    "# Get all input files and sort by athlete id\n",
    "input_files = sorted(os.listdir('./source-files/GRD Pronto/Entrevistas Completas'), key=lambda id: int(id[:2]))\n",
    "input_files_ids = [int(f[:2]) for f in input_files]\n",
    "input_files = [os.getcwd() + '/source-files/GRD Pronto/Entrevistas Completas/' + file for file in input_files]\n",
    "print(\"Interviews (number of documents = %d):\" % len(input_files), input_files_ids, *input_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting input files to JSON with docx2json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all documents into a JSON (dict) list\n",
    "json_arr = [json.loads(docx2json.convert(f)) for f in input_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple pattern recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bold-nonbold': [0, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, 16],\n",
       " 'athlete-name': [1, 3],\n",
       " 'one-one-abbr': [7],\n",
       " 'black-colored': [9],\n",
       " 'all-nonbold-names': [17]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating pattern indexes\n",
    "# I may need to make those recognitions more complex, depending on new patterns and possible match in existing ones\n",
    "json_patterns = {}\n",
    "json_patterns['bold-nonbold'] = [idx for idx, f in enumerate(json_arr) if f['text'][0] == 'Início da transcrição']\n",
    "json_patterns['athlete-name'] = [idx for idx, f in enumerate(json_arr) if 'Atleta: ' in f['text'][0]]\n",
    "json_patterns['one-one-abbr'] = [idx for idx, f in enumerate(json_arr) if 'USP – ' in f['text'][1] or 'USP – ' in f['text'][2]]\n",
    "json_patterns['black-colored'] = [idx for idx, f in enumerate(json_arr) if (' nascida em ' in f['text'][0] or ' nascido em ' in f['text'][0])]\n",
    "json_patterns['all-nonbold-names'] = [idx for idx, f in enumerate(json_arr) if 'Transcrição' in f['text'][0]]\n",
    "json_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing each pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'bold-nonbold' pattern files\n",
    "# Removing 'Início da transcrição' from 'bold-nonbold' pattern files\n",
    "for idx in json_patterns['bold-nonbold']:\n",
    "    if json_arr[idx]['text'][0] == 'Início da transcrição':\n",
    "        del json_arr[idx]['text'][0]\n",
    "    if json_arr[idx]['bold'][0] == 'Início da transcrição':\n",
    "        del json_arr[idx]['bold'][0]\n",
    "    if 'Início da transcrição ' in json_arr[idx]['bold'][0]:\n",
    "        json_arr[idx]['bold'][0] = json_arr[idx]['bold'][0].replace('Início da transcrição ', '')\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'athlete-name' pattern files\n",
    "# Removing names from paragraph authors and trailing spaces, along other redundant information\n",
    "questionset = set(['Kátia', 'Entrevistador']) # to determine if the paragraph is 'bold' or 'nonbold'\n",
    "for idx in json_patterns['athlete-name']:\n",
    "    if 'Atleta: ' in json_arr[idx]['text'][0]:\n",
    "        del json_arr[idx]['text'][0:2]\n",
    "        json_arr[idx]['nonbold'] = []\n",
    "        for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "            json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "            splitpar = s.split(': ', 1)\n",
    "            if (len(splitpar)==2):\n",
    "                json_arr[idx]['text'][idx2] = splitpar[1]\n",
    "                if (splitpar[0] in questionset):\n",
    "                    json_arr[idx]['bold'].append(splitpar[1])\n",
    "                else:\n",
    "                    json_arr[idx]['nonbold'].append(splitpar[1])\n",
    "            else:\n",
    "                json_arr[idx]['nonbold'].append(splitpar[0])\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'one-one-abbr' pattern files\n",
    "# Removing names from paragraph authors and trailing spaces, along other redundant information\n",
    "questionset = set(['USP'])\n",
    "for idx in json_patterns['one-one-abbr']:\n",
    "    if ('USP – ' in json_arr[idx]['text'][1] or 'USP – ' in json_arr[idx]['text'][2]):\n",
    "        del json_arr[idx]['text'][0]\n",
    "        json_arr[idx]['nonbold'] = []\n",
    "        for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "            json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "            splitpar = s.split(' – ', 1)\n",
    "            if (len(splitpar)==2):\n",
    "                json_arr[idx]['text'][idx2] = splitpar[1]\n",
    "                if (splitpar[0] in questionset):\n",
    "                    json_arr[idx]['bold'].append(splitpar[1])\n",
    "                else:\n",
    "                    json_arr[idx]['nonbold'].append(splitpar[1])\n",
    "            else:\n",
    "                json_arr[idx]['nonbold'].append(splitpar[0])\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'black-colored' pattern files\n",
    "check = False\n",
    "for idx in json_patterns['black-colored']:\n",
    "    json_arr[idx]['nonbold'] = []\n",
    "    json_arr[idx]['bold'] = []\n",
    "# Removing unwanted information\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        if (not check):\n",
    "            check = 'Entrevista realizada em ' in s\n",
    "        else:\n",
    "            del json_arr[idx]['text'][0:idx2]\n",
    "            break\n",
    "# Separating questions and answers\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        if (idx2 % 2 == 0):\n",
    "            json_arr[idx]['bold'].append(s)\n",
    "        else:\n",
    "            json_arr[idx]['nonbold'].append(s)\n",
    "# Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'all-nonbold-names' pattern files\n",
    "\n",
    "# Processing 'athlete-name' pattern files\n",
    "# Removing names from paragraph authors and trailing spaces, along other redundant information\n",
    "questionset = set(['Kátia', 'Entrevistador', 'ENTREVISTADORA']) # to determine if the paragraph is 'bold' or 'nonbold'\n",
    "for idx in json_patterns['all-nonbold-names']:\n",
    "    if 'Transcrição ' in json_arr[idx]['text'][0]:\n",
    "        del json_arr[idx]['text'][0]\n",
    "        json_arr[idx]['nonbold'] = []\n",
    "        for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "            json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "            splitpar = s.split(': ', 1)\n",
    "            if (len(splitpar)==2):\n",
    "                json_arr[idx]['text'][idx2] = splitpar[1]\n",
    "                if (splitpar[0] in questionset):\n",
    "                    json_arr[idx]['bold'].append(splitpar[1])\n",
    "                else:\n",
    "                    json_arr[idx]['nonbold'].append(splitpar[1])\n",
    "            else:\n",
    "                json_arr[idx]['nonbold'].append(splitpar[0])\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating META Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate, remove punctation and split into single words for each interview\n",
    "# Create set of tokens for each interview, removing stopwords in the process\n",
    "interviews_split = []\n",
    "tokens = []\n",
    "for idx, x in enumerate(json_arr):\n",
    "    interviews_split.append({\n",
    "        'text': nltk.tokenize.word_tokenize('\\n'.join(x['text']).lower(), language='portuguese'),\n",
    "        'q': nltk.tokenize.word_tokenize('\\n'.join(x['bold']).lower(), language='portuguese'),\n",
    "        'a': nltk.tokenize.word_tokenize('\\n'.join(x['nonbold']).lower(), language='portuguese')\n",
    "    })\n",
    "    tokens.append({\n",
    "        'text': set(interviews_split[idx]['text']) - set(nltk.corpus.stopwords.words('portuguese')),\n",
    "        'q': set(interviews_split[idx]['q']) - set(nltk.corpus.stopwords.words('portuguese')),\n",
    "        'a': set(interviews_split[idx]['a']) - set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating bag-of-words (stemmed or not) for insertion at the database\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('portuguese')\n",
    "# stemmer = nltk.stem.rslp.RSLPStemmer()\n",
    "metas = []\n",
    "for idx, x in enumerate(interviews_split):\n",
    "    meta = {'text': {'bag': {}, 'bag_stemmed': {}}, \n",
    "                'q': {'bag': {}, 'bag_stemmed': {}}, \n",
    "                'a': {'bag': {}, 'bag_stemmed': {}}}\n",
    "    for token in tokens[idx]['text']:\n",
    "        meta['text']['bag'][token] = x['text'].count(token)\n",
    "        meta['text']['bag_stemmed'][stemmer.stem(token)] = meta['text']['bag_stemmed'].get(stemmer.stem(token), 0) + meta['text']['bag'][token]\n",
    "    for token in tokens[idx]['q']:\n",
    "        meta['q']['bag'][token] = x['q'].count(token)\n",
    "        meta['q']['bag_stemmed'][stemmer.stem(token)] = meta['q']['bag_stemmed'].get(stemmer.stem(token), 0) + meta['q']['bag'][token]\n",
    "    for token in tokens[idx]['a']:\n",
    "        meta['a']['bag'][token] = x['a'].count(token)\n",
    "        meta['a']['bag_stemmed'][stemmer.stem(token)] = meta['a']['bag_stemmed'].get(stemmer.stem(token), 0) + meta['a']['bag'][token]\n",
    "    metas.append(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion of data into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "for idx, data in enumerate(json_arr):\n",
    "    cur.execute(\"\"\"INSERT INTO interviews (id, texto, perguntas, respostas) VALUES (%(id)s, %(texto)s, %(perguntas)s, %(respostas)s) ON CONFLICT DO NOTHING;\"\"\",\n",
    "               {'id': input_files_ids[idx], \n",
    "                'texto': '\\n'.join(data['text']), \n",
    "                'perguntas': data['bold'],\n",
    "                'respostas': data['nonbold']\n",
    "               })\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up database connection for performance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing full-text search tools and comparing META information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT ID, tsvector_to_array(TO_TSVECTOR('portuguese', texto)) FROM interviews ORDER BY ID ASC;\"\"\")\n",
    "dbout = cur.fetchall()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  19\n",
      "Lista A (in postgres, not in metas):\n",
      "06\n",
      "1\n",
      "2004\n",
      "60\n",
      "68\n",
      "infant\n",
      "vic\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "1,68\n",
      "``\n",
      "06:24\n",
      ")\n",
      "%\n",
      ",\n",
      "é.\n",
      "”\n",
      "1,60\n",
      ".\n",
      "“\n",
      "91.\n",
      "2004.\n",
      "(\n",
      "vice-\n",
      "?\n",
      "\n",
      "ID:  20\n",
      "Lista A (in postgres, not in metas):\n",
      "2005\n",
      "74\n",
      "74m\n",
      "american\n",
      "anti\n",
      "pré\n",
      "seguranc\n",
      "Lista B (in metas, not in postgres):\n",
      "...\n",
      "–\n",
      "2005.\n",
      "‘\n",
      "segur\n",
      "15.\n",
      "anti-\n",
      ",\n",
      "é.\n",
      "”\n",
      ".\n",
      "“\n",
      "1,74\n",
      "2003.\n",
      "’\n",
      "1,74m\n",
      "?\n",
      "17.\n",
      "\n",
      "ID:  21\n",
      "Lista A (in postgres, not in metas):\n",
      "08\n",
      "14\n",
      "2.000\n",
      "41\n",
      "44\n",
      "american\n",
      "inseguranc\n",
      "lo\n",
      "pan\n",
      "vizinhanc\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "82.\n",
      ")\n",
      "vizinh\n",
      "41:44\n",
      ",\n",
      "!\n",
      "14:08\n",
      "é.\n",
      "”\n",
      ".\n",
      "“\n",
      "2.000.\n",
      "97.\n",
      "(\n",
      "?\n",
      "2002.\n",
      "insegur\n",
      "\n",
      "ID:  22\n",
      "Lista A (in postgres, not in metas):\n",
      "-14\n",
      "-5\n",
      "-60\n",
      "50\n",
      "lest\n",
      "lideranc\n",
      "milk\n",
      "oest\n",
      "shak\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "10:30\n",
      "‘\n",
      "%\n",
      ",\n",
      "50-60\n",
      "13-14\n",
      "”\n",
      ".\n",
      "alucinando.\n",
      "“\n",
      "4-5\n",
      "lider\n",
      "2004.\n",
      "’\n",
      "?\n",
      "\n",
      "ID:  23\n",
      "Lista A (in postgres, not in metas):\n",
      "..\n",
      "16\n",
      "comilanc\n",
      "esperanc\n",
      "execuçã\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "no..\n",
      "e´muit\n",
      "15,16,17,18\n",
      "execu\n",
      "comil\n",
      "não..\n",
      ",\n",
      "!\n",
      "é.\n",
      ".\n",
      "me..\n",
      "e.\n",
      "faziam..\n",
      "2003.\n",
      "?\n",
      "ai..\n",
      "7.\n",
      "\n",
      "ID:  24\n",
      "Lista A (in postgres, not in metas):\n",
      "..\n",
      "18\n",
      "82\n",
      "83\n",
      "imper\n",
      "mielit\n",
      "perióst\n",
      "pré\n",
      "seguranc\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      ")\n",
      "16,17.\n",
      "82,83.\n",
      ",\n",
      "!\n",
      "é.\n",
      ".\n",
      "imper..\n",
      "e.\n",
      "77.\n",
      "a..é\n",
      "(\n",
      "26.\n",
      "18.\n",
      "’\n",
      "?\n",
      "que..\n",
      "sem..\n",
      "82,83\n",
      "\n",
      "ID:  25\n",
      "Lista A (in postgres, not in metas):\n",
      "..\n",
      "00\n",
      "12\n",
      "2016\n",
      "800\n",
      "esperanc\n",
      "seguranc\n",
      "vic\n",
      "Lista B (in metas, not in postgres):\n",
      "2016.\n",
      "...\n",
      ")\n",
      ",\n",
      "[\n",
      "!\n",
      "2009.\n",
      "é.\n",
      ",12\n",
      ".\n",
      "800,00\n",
      "2004.\n",
      "2010.\n",
      "(\n",
      "2.004.\n",
      "]\n",
      "?\n",
      "$\n",
      "de..\n",
      "\n",
      "ID:  26\n",
      "Lista A (in postgres, not in metas):\n",
      "16\n",
      "16.52\n",
      "2004\n",
      "25\n",
      "30min\n",
      "40\n",
      "52min\n",
      "american\n",
      "feir\n",
      "sext\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "–\n",
      "4:16\n",
      "-\n",
      "2:30min\n",
      "16.52min\n",
      "2004.\n",
      "***id\n",
      ";\n",
      ")\n",
      "1:30min\n",
      "2:52min\n",
      "(\n",
      ",\n",
      "!\n",
      "9:25\n",
      "***\n",
      "?\n",
      "7:40\n",
      "”\n",
      "***sed\n",
      ".\n",
      "“\n",
      "\n",
      "ID:  27\n",
      "Lista A (in postgres, not in metas):\n",
      "2008\n",
      "american\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      ")\n",
      ",\n",
      "!\n",
      "é.\n",
      "”\n",
      ".\n",
      "“\n",
      "(\n",
      "2008.\n",
      "?\n",
      ".a\n",
      "\n",
      "ID:  28\n",
      "Lista A (in postgres, not in metas):\n",
      "06\n",
      "22\n",
      "23\n",
      "35\n",
      "d\n",
      "esperanc\n",
      "facil\n",
      "lo\n",
      "los\n",
      "tranquil\n",
      "visit\n",
      "Lista B (in metas, not in postgres):\n",
      "aqui.\n",
      "fortalecimento.\n",
      ":\n",
      "que.\n",
      "...\n",
      "atleta.\n",
      "12.\n",
      "23:13\n",
      "93.\n",
      "tranquila.\n",
      "tudo.\n",
      "acabe.\n",
      "olimpíada.\n",
      "treinar.\n",
      "nanana.\n",
      ",\n",
      "[\n",
      "!\n",
      "estudar.\n",
      "ginástica.\n",
      "é.\n",
      "”\n",
      "2012.\n",
      "nada.\n",
      "dá.\n",
      "vida.\n",
      ".\n",
      "agora.\n",
      "“\n",
      "penso.\n",
      "23:06\n",
      "tal.\n",
      "doendo.\n",
      "d=no\n",
      "]\n",
      "99.\n",
      "não.\n",
      "assim.\n",
      "?\n",
      "apoio.\n",
      "facilmente.\n",
      "magra.\n",
      "continuar.\n",
      "22:35\n",
      "2006.\n",
      "\n",
      "ID:  29\n",
      "Lista A (in postgres, not in metas):\n",
      "00\n",
      "10\n",
      "2000\n",
      "30\n",
      "41não\n",
      "7\n",
      "Lista B (in metas, not in postgres):\n",
      "...\n",
      ")\n",
      "10:30\n",
      "2005.\n",
      "(\n",
      "15.\n",
      "2000.\n",
      "2003.\n",
      ",\n",
      "!\n",
      "7:00\n",
      "?\n",
      "é.\n",
      "41nã\n",
      "6:00\n",
      "7:30\n",
      ".\n",
      "uma..\n",
      "\n",
      "ID:  30\n",
      "Lista A (in postgres, not in metas):\n",
      "00\n",
      "09\n",
      "1.200\n",
      "21\n",
      "27\n",
      "28\n",
      "38\n",
      "39\n",
      "40\n",
      "400\n",
      "50\n",
      "56\n",
      "58\n",
      "bar\n",
      "comp\n",
      "fo\n",
      "re\n",
      "ó\n",
      "Lista B (in metas, not in postgres):\n",
      "00:39:38\n",
      "...\n",
      "00:09:21\n",
      ")\n",
      ",\n",
      "[\n",
      "comp..\n",
      "2007.\n",
      "é.\n",
      "da..\n",
      "”\n",
      "re..\n",
      "00:28:27\n",
      ".\n",
      "“\n",
      "00:28:56\n",
      "1.200,00.\n",
      "00:28:50\n",
      "e.\n",
      "22.\n",
      "bar..\n",
      "(\n",
      "2008.\n",
      "]\n",
      "22,23\n",
      "fo..\n",
      "00:26:58\n",
      "?\n",
      "$\n",
      "que..\n",
      "eu..\n",
      "00:40:12\n",
      "400,00\n",
      "ó.\n",
      "uma..\n",
      "\n",
      "ID:  31\n",
      "Lista A (in postgres, not in metas):\n",
      "las\n",
      "sul\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      ",\n",
      "?\n",
      "é.\n",
      "”\n",
      ".\n",
      "“\n",
      "\n",
      "ID:  32\n",
      "Lista A (in postgres, not in metas):\n",
      "..\n",
      "00\n",
      "04\n",
      "12\n",
      "18\n",
      "30\n",
      "31\n",
      "esperanc\n",
      "nor\n",
      "pré\n",
      "seguranc\n",
      "Lista B (in metas, not in postgres):\n",
      "desse..\n",
      "...\n",
      "com..\n",
      "2011.\n",
      "e´o\n",
      "todo..\n",
      ")\n",
      "não..\n",
      ",\n",
      "[\n",
      "!\n",
      "7:00\n",
      "12:00\n",
      "é.\n",
      "nor..\n",
      "14:30\n",
      ".\n",
      "tem..\n",
      "(\n",
      "00:04:31\n",
      "]\n",
      "6:30\n",
      "?\n",
      "aque..\n",
      "18:00\n",
      "de..\n",
      "2006.\n",
      "\n",
      "ID:  33\n",
      "Lista A (in postgres, not in metas):\n",
      "86\n",
      "96\n",
      "american\n",
      "pan\n",
      "seguranc\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "segur\n",
      ",\n",
      "?\n",
      "96.\n",
      "86.\n",
      "”\n",
      ".\n",
      "“\n",
      "\n",
      "ID:  34\n",
      "Lista A (in postgres, not in metas):\n",
      "..\n",
      "400\n",
      "american\n",
      "ex\n",
      "execuçã\n",
      "regr\n",
      "tilt\n",
      "treinador\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      ")\n",
      "execu\n",
      "%\n",
      ",\n",
      "é.\n",
      "”\n",
      "400.\n",
      ".\n",
      "“\n",
      "-\n",
      "2010.\n",
      "(\n",
      "2003.\n",
      "?\n",
      "regrada..\n",
      "\n",
      "ID:  41\n",
      "Lista A (in postgres, not in metas):\n",
      "00\n",
      "1987\n",
      "600\n",
      "conf\n",
      "inseguranc\n",
      "lo\n",
      "reproduçã\n",
      "Lista B (in metas, not in postgres):\n",
      "...\n",
      ")\n",
      "não..\n",
      ",\n",
      "!\n",
      "1987.\n",
      "confe..\n",
      "é.\n",
      "600,00\n",
      "pra..\n",
      ".\n",
      "e.\n",
      "87.\n",
      "(\n",
      "’\n",
      "?\n",
      "$\n",
      "que..\n",
      "insegur\n",
      "reprodu\n",
      "\n",
      "ID:  42\n",
      "Lista A (in postgres, not in metas):\n",
      "american\n",
      "graduaçã\n",
      "infantil\n",
      "pré\n",
      "pós\n",
      "sul\n",
      "Lista B (in metas, not in postgres):\n",
      ":\n",
      "...\n",
      "2004.\n",
      ")\n",
      "‘\n",
      "(\n",
      "’\n",
      ",\n",
      "!\n",
      "?\n",
      "17.\n",
      "”\n",
      ".\n",
      "“\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differences between my extraction method and the one done by the DBMS\n",
    "for idx, i in enumerate(dbout):\n",
    "    print(\"ID: \", i[0])\n",
    "    print(\"Lista A (in postgres, not in metas):\")\n",
    "    for j in i[1]:\n",
    "        if j not in list(metas[idx]['text']['bag_stemmed'].keys()):\n",
    "            print(j)\n",
    "    print(\"Lista B (in metas, not in postgres):\")\n",
    "    for j in list(metas[idx]['text']['bag_stemmed'].keys()):\n",
    "        if j not in i[1]:\n",
    "            print(j)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

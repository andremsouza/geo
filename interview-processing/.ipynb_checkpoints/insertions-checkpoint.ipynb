{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview processing and insertion into test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2json\n",
    "import json\n",
    "import psycopg2\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from os import listdir, getcwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=35.192.163.239 port=5432 dbname=bdSisGeo user=%s password=%s\" % (input(\"User: \"), getpass.getpass(\"Password: \")))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing interviews with simple pattern recognition / standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviews (number of documents = 18):\n",
      "[19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 41, 42]\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/19 - Camila Ferezin do Amarante ( Ginastica ritmica ).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/20 - Natália Scherer Eidt (Ginastica Ritmica) .docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/21 - Flavia Cristina de Faria (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/22 - Alessandra Ferezin Guidugli (Ginástica Rítmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/23 - Thalita Nakadomari (Ginastica  Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/24 -  Dayane Camilo.docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/25 - larissa barata (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/26 - Fernanda Cavalieri (Ginástica Ritimica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/27 - Ana Maria Maciel (Ginastica Ritimica ).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/28 - Tayanne Coelho Montovanelli (ginastica-ritmica) .docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/29 - Jeniffer Celeste (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/30 - Luana Faro (Ginástica Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/31 -  Daniela Aleixo.docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/32 - Luisa Harumi Matsuo (Ginastica ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/33 - Marcela Oliveira (Ginástica Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/34 - Nicole Romme Muller (Ginastica  Ritimica ).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/41 - Rosane Favilla (Ginastica Ritmica).docx\n",
      "/home/andre/Documents/ic/interview-processing/source-files/GRD Pronto/Entrevistas Completas/42 - Marta Cristina Schonhorst (ginastica-ritmica).docx\n"
     ]
    }
   ],
   "source": [
    "# Get all input files and sort by athlete id\n",
    "input_files = sorted(listdir('./source-files/GRD Pronto/Entrevistas Completas'), key=lambda id: int(id[:2]))\n",
    "input_files_ids = [int(f[:2]) for f in input_files]\n",
    "input_files = [getcwd() + '/source-files/GRD Pronto/Entrevistas Completas/' + file for file in input_files]\n",
    "print(\"Interviews (number of documents = %d):\" % len(input_files), input_files_ids, *input_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting input files to JSON with docx2json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all documents into a JSON (dict) list\n",
    "json_arr = [json.loads(docx2json.convert(f)) for f in input_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple pattern recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bold-nonbold': [0, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, 16],\n",
       " 'athlete-name': [1, 3],\n",
       " 'one-one-abbr': [7],\n",
       " 'black-colored': [9],\n",
       " 'all-nonbold-names': [17]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating pattern indexes\n",
    "# I may need to make those recognitions more complex, depending on new patterns and possible match in existing ones\n",
    "json_patterns = {}\n",
    "json_patterns['bold-nonbold'] = [idx for idx, f in enumerate(json_arr) if f['text'][0] == 'Início da transcrição']\n",
    "json_patterns['athlete-name'] = [idx for idx, f in enumerate(json_arr) if 'Atleta: ' in f['text'][0]]\n",
    "json_patterns['one-one-abbr'] = [idx for idx, f in enumerate(json_arr) if 'USP – ' in f['text'][1] or 'USP – ' in f['text'][2]]\n",
    "json_patterns['black-colored'] = [idx for idx, f in enumerate(json_arr) if (' nascida em ' in f['text'][0] or ' nascido em ' in f['text'][0])]\n",
    "json_patterns['all-nonbold-names'] = [idx for idx, f in enumerate(json_arr) if 'Transcrição' in f['text'][0]]\n",
    "json_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing each pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'bold-nonbold' pattern files\n",
    "# Removing 'Início da transcrição' from 'bold-nonbold' pattern files\n",
    "for idx in json_patterns['bold-nonbold']:\n",
    "    if json_arr[idx]['text'][0] == 'Início da transcrição':\n",
    "        del json_arr[idx]['text'][0]\n",
    "    if json_arr[idx]['bold'][0] == 'Início da transcrição':\n",
    "        del json_arr[idx]['bold'][0]\n",
    "    if 'Início da transcrição ' in json_arr[idx]['bold'][0]:\n",
    "        json_arr[idx]['bold'][0] = json_arr[idx]['bold'][0].replace('Início da transcrição ', '')\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'athlete-name' pattern files\n",
    "# Removing names from paragraph authors and trailing spaces, along other redundant information\n",
    "questionset = set(['Kátia', 'Entrevistador']) # to determine if the paragraph is 'bold' or 'nonbold'\n",
    "for idx in json_patterns['athlete-name']:\n",
    "    if 'Atleta: ' in json_arr[idx]['text'][0]:\n",
    "        del json_arr[idx]['text'][0:2]\n",
    "        json_arr[idx]['nonbold'] = []\n",
    "        for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "            json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "            splitpar = s.split(': ', 1)\n",
    "            if (len(splitpar)==2):\n",
    "                json_arr[idx]['text'][idx2] = splitpar[1]\n",
    "                if (splitpar[0] in questionset):\n",
    "                    json_arr[idx]['bold'].append(splitpar[1])\n",
    "                else:\n",
    "                    json_arr[idx]['nonbold'].append(splitpar[1])\n",
    "            else:\n",
    "                json_arr[idx]['nonbold'].append(splitpar[0])\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'one-one-abbr' pattern files\n",
    "# Removing names from paragraph authors and trailing spaces, along other redundant information\n",
    "questionset = set(['USP'])\n",
    "for idx in json_patterns['one-one-abbr']:\n",
    "    if ('USP – ' in json_arr[idx]['text'][1] or 'USP – ' in json_arr[idx]['text'][2]):\n",
    "        del json_arr[idx]['text'][0]\n",
    "        json_arr[idx]['nonbold'] = []\n",
    "        for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "            json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "            splitpar = s.split(' – ', 1)\n",
    "            if (len(splitpar)==2):\n",
    "                json_arr[idx]['text'][idx2] = splitpar[1]\n",
    "                if (splitpar[0] in questionset):\n",
    "                    json_arr[idx]['bold'].append(splitpar[1])\n",
    "                else:\n",
    "                    json_arr[idx]['nonbold'].append(splitpar[1])\n",
    "            else:\n",
    "                json_arr[idx]['nonbold'].append(splitpar[0])\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'black-colored' pattern files\n",
    "check = False\n",
    "for idx in json_patterns['black-colored']:\n",
    "    json_arr[idx]['nonbold'] = []\n",
    "    json_arr[idx]['bold'] = []\n",
    "# Removing unwanted information\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        if (not check):\n",
    "            check = 'Entrevista realizada em ' in s\n",
    "        else:\n",
    "            del json_arr[idx]['text'][0:idx2]\n",
    "            break\n",
    "# Separating questions and answers\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        if (idx2 % 2 == 0):\n",
    "            json_arr[idx]['bold'].append(s)\n",
    "        else:\n",
    "            json_arr[idx]['nonbold'].append(s)\n",
    "# Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 'all-nonbold-names' pattern files\n",
    "\n",
    "# Processing 'athlete-name' pattern files\n",
    "# Removing names from paragraph authors and trailing spaces, along other redundant information\n",
    "questionset = set(['Kátia', 'Entrevistador', 'ENTREVISTADORA']) # to determine if the paragraph is 'bold' or 'nonbold'\n",
    "for idx in json_patterns['all-nonbold-names']:\n",
    "    if 'Transcrição ' in json_arr[idx]['text'][0]:\n",
    "        del json_arr[idx]['text'][0]\n",
    "        json_arr[idx]['nonbold'] = []\n",
    "        for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "            json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "            splitpar = s.split(': ', 1)\n",
    "            if (len(splitpar)==2):\n",
    "                json_arr[idx]['text'][idx2] = splitpar[1]\n",
    "                if (splitpar[0] in questionset):\n",
    "                    json_arr[idx]['bold'].append(splitpar[1])\n",
    "                else:\n",
    "                    json_arr[idx]['nonbold'].append(splitpar[1])\n",
    "            else:\n",
    "                json_arr[idx]['nonbold'].append(splitpar[0])\n",
    "#   Removing trailing spaces\n",
    "    for idx2, s in enumerate(json_arr[idx]['text']):\n",
    "        json_arr[idx]['text'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['bold']):\n",
    "        json_arr[idx]['bold'][idx2] = s.strip(' ')\n",
    "    for idx2, s in enumerate(json_arr[idx]['nonbold']):\n",
    "        json_arr[idx]['nonbold'][idx2] = s.strip(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating META Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2json\n",
    "import json\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate, remove punctation and split into single words for each interview\n",
    "# Create set of tokens for each interview, removing stopwords in the process\n",
    "interviews_split = copy.deepcopy(json_arr)\n",
    "tokens = []\n",
    "pattern = re.compile('([^\\s\\w|\\-]|_)+')\n",
    "for idx, x in enumerate(interviews_split):\n",
    "    interviews_split[idx]['text'] = pattern.sub(' ', '\\n'.join(x['text'])).lower().split()\n",
    "    interviews_split[idx]['q'] = pattern.sub(' ', '\\n'.join(x['bold'])).lower().split()\n",
    "    interviews_split[idx]['a'] = pattern.sub(' ', '\\n'.join(x['nonbold'])).lower().split()\n",
    "    tokens.append({'text': set(interviews_split[idx]['text']) - set(nltk.corpus.stopwords.words('portuguese')),\n",
    "                  'q': set(interviews_split[idx]['q']) - set(nltk.corpus.stopwords.words('portuguese')),\n",
    "                  'a': set(interviews_split[idx]['a']) - set(nltk.corpus.stopwords.words('portuguese'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating bag-of-words (stemmed or not) for insertion at the database\n",
    "stemmer = SnowballStemmer('portuguese')\n",
    "metas = []\n",
    "for idx, x in enumerate(interviews_split):\n",
    "    meta = {'text': {'bag': {}, 'bag_stemmed': {}}, \n",
    "                'q': {'bag': {}, 'bag_stemmed': {}}, \n",
    "                'a': {'bag': {}, 'bag_stemmed': {}}}\n",
    "    for token in tokens[idx]['text']:\n",
    "        meta['text']['bag'][token] = x['text'].count(token)\n",
    "        meta['text']['bag_stemmed'][stemmer.stem(token)] = meta['text']['bag_stemmed'].get(stemmer.stem(token), 0) + meta['text']['bag'][token]\n",
    "    for token in tokens[idx]['q']:\n",
    "        meta['q']['bag'][token] = x['q'].count(token)\n",
    "        meta['q']['bag_stemmed'][stemmer.stem(token)] = meta['q']['bag_stemmed'].get(stemmer.stem(token), 0) + meta['q']['bag'][token]\n",
    "    for token in tokens[idx]['a']:\n",
    "        meta['a']['bag'][token] = x['a'].count(token)\n",
    "        meta['a']['bag_stemmed'][stemmer.stem(token)] = meta['a']['bag_stemmed'].get(stemmer.stem(token), 0) + meta['a']['bag'][token]\n",
    "    metas.append(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion of data into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "for idx, data in enumerate(json_arr):\n",
    "    cur.execute(\"\"\"INSERT INTO \"tbEntrevista\" VALUES (%(id)s, %(texto)s, %(perguntas)s, %(respostas)s, %(meta)s) ON CONFLICT DO NOTHING;\"\"\",\n",
    "               {'id': input_files_ids[idx], \n",
    "                'texto': '\\n'.join(data['text']), \n",
    "                'perguntas': data['bold'],\n",
    "                'respostas': data['nonbold'],\n",
    "                'meta': json.dumps(metas[idx])})\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up database connection for performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=35.192.163.239 port=5432 dbname=testdb user=%s password=%s\" % (input(\"User: \"), getpass.getpass(\"Password: \")))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting data into test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting into each one of the testing tables\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"DELETE FROM textarrtest;\"\"\")\n",
    "cur.execute(\"\"\"DELETE FROM jsonbtest;\"\"\")\n",
    "cur.execute(\"\"\"DELETE FROM jsontest;\"\"\")\n",
    "for idx, data in enumerate(json_arr):\n",
    "    cur.execute(\n",
    "        \"\"\"INSERT INTO textarrtest VALUES (%(id)s, %(texto)s, %(perguntas)s, %(respostas)s) ON CONFLICT DO NOTHING;\"\"\",\n",
    "        {\n",
    "            'id': input_files_ids[idx],\n",
    "            'texto': '\\n'.join(data['text']),\n",
    "            'perguntas': data['bold'],\n",
    "            'respostas': data['nonbold']\n",
    "        }\n",
    "    )\n",
    "    cur.execute(\n",
    "        \"\"\"INSERT INTO jsonbtest VALUES (%(id)s, %(interview)s) ON CONFLICT DO NOTHING;\"\"\",\n",
    "        {\n",
    "            'id': input_files_ids[idx],\n",
    "            'interview': json.dumps({'text': '\\n'.join(data['text']), 'bold': data['bold'], 'nonbold': data['nonbold']})\n",
    "        }\n",
    "    )\n",
    "    cur.execute(\n",
    "        \"\"\"INSERT INTO jsontest VALUES (%(id)s, %(interview)s) ON CONFLICT DO NOTHING;\"\"\",\n",
    "        {\n",
    "            'id': input_files_ids[idx],\n",
    "            'interview': json.dumps({'text': '\\n'.join(data['text']), 'bold': data['bold'], 'nonbold': data['nonbold']})\n",
    "        }\n",
    "    )\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing full-text search tools and comparing META information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT ID, tsvector_to_array(TO_TSVECTOR('portuguese', texto)) FROM TEXTARRTEST ORDER BY ID ASC;\"\"\")\n",
    "dbout = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  19\n",
      "Lista A:\n",
      "infant\n",
      "vic\n",
      "Lista B:\n",
      "vice-\n",
      "\n",
      "ID:  20\n",
      "Lista A:\n",
      "1.400\n",
      "american\n",
      "anti\n",
      "pré\n",
      "seguranc\n",
      "Lista B:\n",
      "anti-\n",
      "segur\n",
      "\n",
      "ID:  21\n",
      "Lista A:\n",
      "2.000\n",
      "american\n",
      "inseguranc\n",
      "lo\n",
      "pan\n",
      "vizinhanc\n",
      "Lista B:\n",
      "000\n",
      "vizinh\n",
      "2\n",
      "insegur\n",
      "\n",
      "ID:  22\n",
      "Lista A:\n",
      "-14\n",
      "-5\n",
      "-60\n",
      "1.000\n",
      "3.000\n",
      "50\n",
      "lest\n",
      "lideranc\n",
      "milk\n",
      "oest\n",
      "shak\n",
      "Lista B:\n",
      "13-14\n",
      "000\n",
      "4-5\n",
      "50-60\n",
      "lider\n",
      "\n",
      "ID:  23\n",
      "Lista A:\n",
      "..\n",
      "comilanc\n",
      "esperanc\n",
      "execuçã\n",
      "Lista B:\n",
      "comil\n",
      "execu\n",
      "\n",
      "ID:  24\n",
      "Lista A:\n",
      "..\n",
      "2.000\n",
      "mielit\n",
      "perióst\n",
      "pré\n",
      "seguranc\n",
      "Lista B:\n",
      "000\n",
      "2\n",
      "\n",
      "ID:  25\n",
      "Lista A:\n",
      "..\n",
      "2.003\n",
      "2.004\n",
      "30/03/1987\n",
      "esperanc\n",
      "seguranc\n",
      "vic\n",
      "Lista B:\n",
      "30\n",
      "03\n",
      "004\n",
      "003\n",
      "1987\n",
      "\n",
      "ID:  26\n",
      "Lista A:\n",
      "16.52\n",
      "2003/2004\n",
      "american\n",
      "feir\n",
      "sext\n",
      "Lista B:\n",
      "-\n",
      "\n",
      "ID:  27\n",
      "Lista A:\n",
      "american\n",
      "Lista B:\n",
      "\n",
      "ID:  28\n",
      "Lista A:\n",
      "esperanc\n",
      "lo\n",
      "los\n",
      "visit\n",
      "Lista B:\n",
      "\n",
      "ID:  29\n",
      "Lista A:\n",
      "41não\n",
      "Lista B:\n",
      "41nã\n",
      "\n",
      "ID:  30\n",
      "Lista A:\n",
      "..\n",
      "1.200\n",
      "1.800\n",
      "Lista B:\n",
      "800\n",
      "\n",
      "ID:  31\n",
      "Lista A:\n",
      "las\n",
      "sul\n",
      "Lista B:\n",
      "\n",
      "ID:  32\n",
      "Lista A:\n",
      "..\n",
      "esperanc\n",
      "pré\n",
      "seguranc\n",
      "Lista B:\n",
      "\n",
      "ID:  33\n",
      "Lista A:\n",
      "american\n",
      "pan\n",
      "seguranc\n",
      "Lista B:\n",
      "segur\n",
      "\n",
      "ID:  34\n",
      "Lista A:\n",
      "..\n",
      "2002/2003\n",
      "a.e.r\n",
      "american\n",
      "ex\n",
      "execuçã\n",
      "tilt\n",
      "treinador\n",
      "Lista B:\n",
      "execu\n",
      "-\n",
      "2002\n",
      "r\n",
      "\n",
      "ID:  41\n",
      "Lista A:\n",
      "9.20\n",
      "inseguranc\n",
      "lo\n",
      "reproduçã\n",
      "Lista B:\n",
      "insegur\n",
      "reprodu\n",
      "\n",
      "ID:  42\n",
      "Lista A:\n",
      "american\n",
      "graduaçã\n",
      "infantil\n",
      "pré\n",
      "pós\n",
      "sul\n",
      "Lista B:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differences between my extraction method and the one done by the DBMS\n",
    "for idx, i in enumerate(dbout):\n",
    "    print(\"ID: \", i[0])\n",
    "    print(\"Lista A:\")\n",
    "    for j in i[1]:\n",
    "        if j not in list(metas[idx]['text']['bag_stemmed'].keys()):\n",
    "            print(j)\n",
    "    print(\"Lista B:\")\n",
    "    for j in list(metas[idx]['text']['bag_stemmed'].keys()):\n",
    "        if j not in i[1]:\n",
    "            print(j)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
